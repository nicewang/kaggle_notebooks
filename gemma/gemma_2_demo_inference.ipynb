{"metadata":{"accelerator":"GPU","colab":{"toc_visible":true,"provenance":[]},"google":{"image_path":"/site-assets/images/marketing/gemma.png","keywords":["examples","gemma","python","quickstart","text"]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.15","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":85986,"sourceType":"modelInstanceVersion","modelInstanceId":72246,"modelId":78150}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##### Copyright 2024 Google LLC.","metadata":{"id":"Tce3stUlHN0L"}},{"cell_type":"markdown","source":"#### **Accelerator:** TPU VM v3-8","metadata":{}},{"cell_type":"code","source":"#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.","metadata":{"id":"tuOe1ymfHZPu","execution":{"iopub.status.busy":"2024-10-15T15:25:37.470863Z","iopub.execute_input":"2024-10-15T15:25:37.471827Z","iopub.status.idle":"2024-10-15T15:25:37.475868Z","shell.execute_reply.started":"2024-10-15T15:25:37.471790Z","shell.execute_reply":"2024-10-15T15:25:37.474804Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"<table class=\"tfo-notebook-buttons\" align=\"left\">\n  <td>\n    <a target=\"_blank\" href=\"https://ai.google.dev/gemma/docs/keras_inference\"><img src=\"https://ai.google.dev/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />View on ai.google.dev</a>\n  </td>\n    <td>\n    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/keras_inference.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/google/generative-ai-docs/main/site/en/gemma/docs/keras_inference.ipynb\"><img src=\"https://ai.google.dev/images/cloud-icon.svg\" width=\"40\" />Open in Vertex AI</a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://github.com/google/generative-ai-docs/blob/main/site/en/gemma/docs/keras_inference.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n  </td>\n</table>","metadata":{"id":"4qxv4Sn9b8CE"}},{"cell_type":"markdown","source":"# Get started with Gemma using KerasNLP\n\nThis tutorial shows you how to get started with Gemma using [KerasNLP](https://keras.io/keras_nlp/). Gemma is a family of lightweight, state-of-the art open models built from the same research and technology used to create the Gemini models. KerasNLP is a collection of natural language processing (NLP) models implemented in [Keras](https://keras.io/) and runnable on JAX, PyTorch, and TensorFlow.\n\nIn this tutorial, you'll use Gemma to generate text responses to several prompts. If you're new to Keras, you might want to read [Getting started with Keras](https://keras.io/getting_started/) before you begin, but you don't have to. You'll learn more about Keras as you work through this tutorial.","metadata":{"id":"PXNm5_p_oxMF"}},{"cell_type":"markdown","source":"## Setup","metadata":{"id":"mERVCCsGUPIJ"}},{"cell_type":"markdown","source":"### Gemma setup\n\nTo complete this tutorial, you will first need to complete the setup instructions at [Gemma setup](https://ai.google.dev/gemma/docs/setup). The Gemma setup instructions show you how to do the following:\n\nGemma models are hosted by Kaggle. To use Gemma, request access on Kaggle:\n\n- Sign in or register at [kaggle.com](https://www.kaggle.com)\n- Open the [Gemma 2 model card](https://www.kaggle.com/models/google/gemma-2) and select _\"Request Access\"_\n- Complete the consent form and accept the terms and conditions\n","metadata":{"id":"QQ6W7NzRe1VM"}},{"cell_type":"markdown","source":"### Install dependencies\n\nInstall Keras and KerasNLP.","metadata":{"id":"z9oy3QUmXtSd"}},{"cell_type":"code","source":"# # Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n# !pip install -q -U keras-nlp\n# !pip install -q -U \"keras>=3\"","metadata":{"id":"UcGLzDeQ8NwN","execution":{"iopub.status.busy":"2024-10-15T14:35:32.571853Z","iopub.execute_input":"2024-10-15T14:35:32.572157Z","iopub.status.idle":"2024-10-15T14:35:32.575690Z","shell.execute_reply.started":"2024-10-15T14:35:32.572130Z","shell.execute_reply":"2024-10-15T14:35:32.574938Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U keras-nlp\n# Work around an import error with tensorflow-hub. The library is not used.\n!pip install -q -U tensorflow-hub\n# Install tensorflow-cpu so tensorflow does not attempt to access the TPU.\n!pip install -q -U tensorflow-cpu tensorflow-text\n# Install keras 3 last. See https://keras.io/getting_started for details.\n!pip install -q -U keras","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:35:34.990615Z","iopub.execute_input":"2024-10-15T14:35:34.990931Z","iopub.status.idle":"2024-10-15T14:37:16.812254Z","shell.execute_reply.started":"2024-10-15T14:35:34.990905Z","shell.execute_reply":"2024-10-15T14:37:16.811213Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntf-keras 2.16.0 requires tensorflow<2.17,>=2.16, but you have tensorflow 2.17.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import jax\n\njax.devices()","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:37:53.687837Z","iopub.execute_input":"2024-10-15T14:37:53.688208Z","iopub.status.idle":"2024-10-15T14:38:02.098206Z","shell.execute_reply.started":"2024-10-15T14:37:53.688178Z","shell.execute_reply":"2024-10-15T14:38:02.097268Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1729003078.404204      13 common_lib.cc:798] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: ===\nlearning/45eac/tfrc/runtime/common_lib.cc:479\nE1015 14:37:58.443535641     117 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2024-10-15T14:37:58.443514262+00:00\", grpc_status:2}\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"},"metadata":{}}]},{"cell_type":"code","source":"jax.__version__","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:38:08.166858Z","iopub.execute_input":"2024-10-15T14:38:08.167320Z","iopub.status.idle":"2024-10-15T14:38:08.172821Z","shell.execute_reply.started":"2024-10-15T14:38:08.167284Z","shell.execute_reply":"2024-10-15T14:38:08.172026Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'0.4.23'"},"metadata":{}}]},{"cell_type":"code","source":"import keras\n\nkeras.__version__","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:38:30.789533Z","iopub.execute_input":"2024-10-15T14:38:30.789907Z","iopub.status.idle":"2024-10-15T14:38:41.337908Z","shell.execute_reply.started":"2024-10-15T14:38:30.789876Z","shell.execute_reply":"2024-10-15T14:38:41.337132Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"2024-10-15 14:38:36.721395: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-10-15 14:38:36.743891: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-10-15 14:38:36.750782: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'3.6.0'"},"metadata":{}}]},{"cell_type":"code","source":"import time\n\nstart_time = time.perf_counter()\n\nprint(\"Hello, Gemma 2 Demo!\")\n\nend_time = time.perf_counter() \nexecution_time = (end_time - start_time) * 1000 \nprint(f\"Execution Time: {execution_time:.3f} ms\")","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:42:54.481767Z","iopub.execute_input":"2024-10-15T14:42:54.482485Z","iopub.status.idle":"2024-10-15T14:42:54.487825Z","shell.execute_reply.started":"2024-10-15T14:42:54.482429Z","shell.execute_reply":"2024-10-15T14:42:54.487074Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Hello, Gemma 2 Demo!\nExecution Time: 0.129 ms\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Select a backend\n\nKeras is a high-level, multi-framework deep learning API designed for simplicity and ease of use. [Keras 3](https://keras.io/keras_3) lets you choose the backend: TensorFlow, JAX, or PyTorch. All three will work for this tutorial.","metadata":{"id":"Pm5cVOFt5YvZ"}},{"cell_type":"code","source":"import os\n\n# os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"tensorflow\" or \"torch\".\n# The Keras 3 distribution API is only implemented for the JAX backend for now\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n# Pre-allocate 90% of TPU memory to minimize memory fragmentation and allocation\n# overhead\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.9\"","metadata":{"id":"7rS7ryTs5wjf","execution":{"iopub.status.busy":"2024-10-15T14:43:02.941293Z","iopub.execute_input":"2024-10-15T14:43:02.941742Z","iopub.status.idle":"2024-10-15T14:43:02.946892Z","shell.execute_reply.started":"2024-10-15T14:43:02.941703Z","shell.execute_reply":"2024-10-15T14:43:02.945813Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# import gc\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-10-15T14:43:13.861011Z","iopub.execute_input":"2024-10-15T14:43:13.861452Z","iopub.status.idle":"2024-10-15T14:43:13.865333Z","shell.execute_reply.started":"2024-10-15T14:43:13.861398Z","shell.execute_reply":"2024-10-15T14:43:13.864479Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Import packages\n\nImport Keras and KerasNLP.","metadata":{"id":"599765c72722"}},{"cell_type":"code","source":"import keras\nimport keras_nlp","metadata":{"id":"f2fa267d75bc","_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-10-15T14:43:21.374517Z","iopub.execute_input":"2024-10-15T14:43:21.375536Z","iopub.status.idle":"2024-10-15T14:43:21.927473Z","shell.execute_reply.started":"2024-10-15T14:43:21.375494Z","shell.execute_reply":"2024-10-15T14:43:21.926495Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Create a model\n\nKerasNLP provides implementations of many popular [model architectures](https://keras.io/api/keras_nlp/models/). In this tutorial, you'll create a model using `GemmaCausalLM`, an end-to-end Gemma model for causal language modeling. A causal language model predicts the next token based on previous tokens.\n\nCreate the model using the `from_preset` method:","metadata":{"id":"ZsxDCbLN555T"}},{"cell_type":"code","source":"gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_instruct_2b_en\")","metadata":{"id":"yygIK9DEIldp","_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-10-15T14:43:28.249831Z","iopub.execute_input":"2024-10-15T14:43:28.250203Z","iopub.status.idle":"2024-10-15T14:44:30.410517Z","shell.execute_reply.started":"2024-10-15T14:43:28.250171Z","shell.execute_reply":"2024-10-15T14:44:30.409330Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The `GemmaCausalLM.from_preset()` function instantiates the model from a preset architecture and weights. In the code above, the string `\"gemma2_2b_en\"` specifies the preset the Gemma 2 2B model with 2 billion parameters. Gemma models with [7B, 9B, and 27B parameters](/gemma/docs/get_started#models-list) are also available. You can find the code strings for Gemma models in their **Model Variation** listings on [Kaggle](https://www.kaggle.com/models/google/gemma).\n","metadata":{"id":"XrAWvsU6pI0E"}},{"cell_type":"markdown","source":"Note: To run the larger models in Colab, you need access to the premium GPUs available in paid plans. Alternatively, you can perform inferences using Kaggle notebooks or Google Cloud projects.\n","metadata":{"id":"Ij73k0PfUhjE"}},{"cell_type":"markdown","source":"Use `summary` to get more info about the model:","metadata":{"id":"E-cSEjULUhST"}},{"cell_type":"code","source":"gemma_lm.summary()","metadata":{"id":"e5nEbTdApL7W","outputId":"43cbd36c-ffdb-4e00-9b92-2b5b81043b27","execution":{"iopub.status.busy":"2024-10-15T14:44:34.711261Z","iopub.execute_input":"2024-10-15T14:44:34.712259Z","iopub.status.idle":"2024-10-15T14:44:34.742212Z","shell.execute_reply.started":"2024-10-15T14:44:34.712224Z","shell.execute_reply":"2024-10-15T14:44:34.741251Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,614,341,888\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"As you can see from the summary, the model has 2.6 billion trainable parameters.\n\nNote: For purposes of naming the model (\"2B\"), the embedding layer is not counted against the number of parameters.","metadata":{"id":"81KHdRYOrWYm"}},{"cell_type":"markdown","source":"## Generate text\n\nNow it's time to generate some text! The model has a `generate` method that generates text based on a prompt. The optional `max_length` argument specifies the maximum length of the generated sequence.\n\n**Demo-1:** Try it out with the prompt `\"what is keras in 3 bullet points?\"`.","metadata":{"id":"FOBW7piN5-sl"}},{"cell_type":"code","source":"prompt = \"\"\"<start_of_turn>user\nwhat is keras in 3 bullet points?<end_of_turn>\n<start_of_turn>model\n\"\"\"\nstart_time = time.perf_counter()\n\noutput = gemma_lm.generate(prompt, max_length=256)\n\nend_time = time.perf_counter() \nexecution_time = (end_time - start_time) * 1000 \nprint(f\"Inference Time: {execution_time:.3f} ms\")\n\nprint(output)","metadata":{"id":"aae5GHrdpj2_","outputId":"d57568a9-ff32-4dc7-89ef-a0ea2d01027b","execution":{"iopub.status.busy":"2024-10-15T14:44:40.991652Z","iopub.execute_input":"2024-10-15T14:44:40.992002Z","iopub.status.idle":"2024-10-15T14:45:34.267340Z","shell.execute_reply.started":"2024-10-15T14:44:40.991973Z","shell.execute_reply":"2024-10-15T14:45:34.266011Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1729003497.604926    1091 service.cc:146] XLA service 0x7a93e460f7f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1729003497.604996    1091 service.cc:154]   StreamExecutor device (0): Host, Default Version\nI0000 00:00:1729003497.627111    1091 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"Inference Time: 53269.405 ms\n<start_of_turn>user\nwhat is keras in 3 bullet points?<end_of_turn>\n<start_of_turn>model\nHere's a breakdown of Keras in 3 bullet points:\n\n* **High-Level API:** Keras provides a user-friendly interface for building and training neural networks, simplifying the process of working with complex models.\n* **Flexible and Modular:** Keras is built on top of TensorFlow, offering a modular design that allows you to easily integrate with other TensorFlow components or use it independently.\n* **Rapid Prototyping:** Keras excels at rapid prototyping, enabling you to quickly experiment with different architectures and hyperparameters to find optimal solutions for your machine learning tasks. \n<end_of_turn>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Demo-1 Imitation:** Try it out with the prompt `\"Give me some ways to Interlaken from Zurich?\"`.","metadata":{}},{"cell_type":"code","source":"prompt = \"\"\"<start_of_turn>user\nGive me some ways to Interlaken from Zurich?<end_of_turn>\n<start_of_turn>model\n\"\"\"\n\nstart_time = time.perf_counter()\n\noutput = gemma_lm.generate(prompt, max_length=512)\n\nend_time = time.perf_counter() \nexecution_time = (end_time - start_time) * 1000 \nprint(f\"Inference Time: {execution_time:.3f} ms\")\n\nprint(output)","metadata":{"id":"aae5GHrdpj2_","outputId":"d57568a9-ff32-4dc7-89ef-a0ea2d01027b","execution":{"iopub.status.busy":"2024-10-15T14:46:20.048922Z","iopub.execute_input":"2024-10-15T14:46:20.049315Z","iopub.status.idle":"2024-10-15T14:48:25.168267Z","shell.execute_reply.started":"2024-10-15T14:46:20.049283Z","shell.execute_reply":"2024-10-15T14:48:25.167113Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Inference Time: 125113.306 ms\n<start_of_turn>user\nGive me some ways to Interlaken from Zurich?<end_of_turn>\n<start_of_turn>model\nHere are some ways to get from Zurich to Interlaken:\n\n**By Train:**\n\n* **Direct Train:** The most convenient option is taking a direct train from Zurich HB (main station) to Interlaken Ost. This journey takes around 2 hours and 15 minutes. You can find tickets on the Swiss Federal Railways (SBB) website or at the station.\n* **Change in Lucerne:** You can also take a train from Zurich HB to Lucerne, then change to a train to Interlaken Ost. This option might be slightly cheaper but adds a bit more travel time.\n\n**By Bus:**\n\n* **Bus:** Several bus companies offer routes from Zurich to Interlaken. This option is generally more affordable than the train but can take longer, especially during peak season. You can find bus schedules and book tickets on websites like FlixBus or PostAuto.\n\n**By Car:**\n\n* **Driving:** If you prefer to drive, it takes around 2 hours and 30 minutes to reach Interlaken from Zurich. You can use the A1 motorway for the journey. \n\n**Things to consider:**\n\n* **Cost:** Train tickets are generally more expensive than bus tickets, but they are faster and more comfortable.\n* **Time:** Trains are the fastest option, while buses and driving can take longer.\n* **Comfort:** Trains offer more legroom and comfort than buses.\n* **Flexibility:** Driving gives you more flexibility to stop along the way, but it can be more stressful during peak season.\n\n\nI hope this helps! \n<end_of_turn>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Demo-1 Imitation (Chinese):** Try it out with the prompt `\"新疆伊犁的最佳旅游时节和攻略？\"`.","metadata":{}},{"cell_type":"code","source":"prompt = \"\"\"<start_of_turn>user\n新疆伊犁的最佳旅游时节和攻略？<end_of_turn>\n<start_of_turn>model\n\"\"\"\n\nstart_time = time.perf_counter()\n\noutput = gemma_lm.generate(prompt, max_length=1024)\n\nend_time = time.perf_counter() \nexecution_time = (end_time - start_time) * 1000 \nprint(f\"Inference Time: {execution_time:.3f} ms\")\n\nprint(output)","metadata":{"id":"aae5GHrdpj2_","outputId":"d57568a9-ff32-4dc7-89ef-a0ea2d01027b","execution":{"iopub.status.busy":"2024-10-15T15:03:31.105763Z","iopub.execute_input":"2024-10-15T15:03:31.106524Z","iopub.status.idle":"2024-10-15T15:09:28.868966Z","shell.execute_reply.started":"2024-10-15T15:03:31.106490Z","shell.execute_reply":"2024-10-15T15:09:28.867846Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Inference Time: 357756.787 ms\n<start_of_turn>user\n新疆伊犁的最佳旅游时节和攻略？<end_of_turn>\n<start_of_turn>model\n## 新疆伊犁最佳旅游时节和攻略\n\n**最佳旅游时节:**\n\n* **春季 (4月-5月):**  天气温暖，景色宜人，适合观赏花海和草原。\n* **夏季 (6月-8月):**  天气炎热，适合体验伊犁的自然风光和文化活动。\n* **秋季 (9月-10月):**  天气凉爽，适合观赏秋色和体验伊犁的传统节日。\n* **冬季 (11月-3月):**  天气寒冷，适合体验雪景和冰雪活动。\n\n**具体推荐:**\n\n* **春季:**  \n    * **推荐景点:**  \n        * **哈密:**  游览哈密古城，感受古老的文化和历史。\n        * **库尔勒:**  参观库尔勒博物馆，了解伊犁的文化和历史。\n        * **阿克苏:**  体验阿克苏的特色美食和文化活动。\n    * **活动:**  \n        * **观赏花海:**  春季伊犁的草原和山区都呈现出美丽的景色，适合观赏各种花卉。\n        * **体验草原文化:**  参加草原文化体验活动，感受伊犁的传统生活方式。\n* **夏季:**  \n    * **推荐景点:**  \n        * **库尔勒:**  参观库尔勒的自然景观，例如库尔勒湖和库尔勒山。\n        * **哈密:**  游览哈密的自然景观，例如哈密山和哈密湖。\n        * **阿克苏:**  体验阿克苏的特色美食和文化活动。\n    * **活动:**  \n        * **体验草原活动:**  参加草原骑马、野营等活动，感受伊犁的自然风光。\n        * **观赏自然景观:**  夏季伊犁的自然景观更加壮观，适合观赏草原和山脉。\n* **秋季:**  \n    * **推荐景点:**  \n        * **哈密:**  游览哈密的自然景观，例如哈密山和哈密湖。\n        * **库尔勒:**  参观库尔勒的自然景观，例如库尔勒湖和库尔勒山。\n        * **阿克苏:**  体验阿克苏的特色美食和文化活动。\n    * **活动:**  \n        * **观赏秋色:**  秋季伊犁的景色更加美丽，适合观赏秋色和体验伊犁的传统节日。\n        * **体验文化活动:**  参加伊犁的传统节日活动，例如伊犁的传统音乐和舞蹈表演。\n* **冬季:**  \n    * **推荐景点:**  \n        * **库尔勒:**  参观库尔勒的雪景，例如库尔勒雪山和库尔勒雪场。\n        * **哈密:**  游览哈密的雪景，例如哈密雪山和哈密雪场。\n        * **阿克苏:**  体验阿克苏的特色美食和文化活动。\n    * **活动:**  \n        * **体验雪景:**  冬季伊犁的雪景更加壮观，适合体验雪景和冰雪活动。\n        * **体验冬日文化:**  参加伊犁的冬日文化活动，例如伊犁的传统节日和冬日美食。\n\n**攻略:**\n\n* **交通:**  伊犁的交通较为便利，可以乘坐飞机、火车、汽车等交通工具到达。\n* **住宿:**  伊犁的住宿选择丰富，可以选择酒店、民宿、旅馆等。\n* **餐饮:**  伊犁的餐饮选择丰富，可以品尝到伊犁的特色美食，例如羊肉泡馍、烤肉、奶酪等。\n* **安全:**  伊犁的治安良好，但需要注意防范盗窃和诈骗。\n* **语言:**  伊犁的官方语言是汉语，但当地人会说少数民族语言，建议学习一些简单的少数民族语言。\n\n\n希望以上信息对您有所帮助！ \n<end_of_turn>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Demo-2:** Try calling `generate` again with a different prompt.","metadata":{"id":"qH0eFH_DvYwM"}},{"cell_type":"code","source":"start_time = time.perf_counter()\n\nresults = gemma_lm.generate(\"list, in less than 100 words, 5 things you can help me do?\", max_length=128)\n\nend_time = time.perf_counter() \nexecution_time = (end_time - start_time) * 1000 \nprint(f\"Inference Time: {execution_time:.3f} ms\")\n\nprint(results)","metadata":{"id":"VEyTnnNGvgGG","outputId":"bb60544b-52a1-460e-9cf4-44225a9334f2","execution":{"iopub.status.busy":"2024-10-15T14:55:52.853356Z","iopub.execute_input":"2024-10-15T14:55:52.853792Z","iopub.status.idle":"2024-10-15T14:56:27.280585Z","shell.execute_reply.started":"2024-10-15T14:55:52.853758Z","shell.execute_reply":"2024-10-15T14:56:27.279303Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Inference Time: 34421.121 ms\nlist, in less than 100 words, 5 things you can help me do?\n\n1. **Write a poem about a lost cat.**\n2. **Create a list of fun activities for a rainy day.**\n3. **Generate a funny joke.**\n4. **Help me brainstorm ideas for a birthday party.**\n5. **Explain a complex scientific concept in simple terms.** \n\n\n<end_of_turn>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Demo-2 Imitation**","metadata":{}},{"cell_type":"code","source":"start_time = time.perf_counter()\n\nresults = gemma_lm.generate(\"table, in less than 100 words, 3 most impressive final matches of the World Cup?\", max_length=128)\n\nend_time = time.perf_counter() \nexecution_time = (end_time - start_time) * 1000 \nprint(f\"Inference Time: {execution_time:.3f} ms\")\n\nprint(results)","metadata":{"id":"VEyTnnNGvgGG","outputId":"bb60544b-52a1-460e-9cf4-44225a9334f2","execution":{"iopub.status.busy":"2024-10-15T14:58:05.522357Z","iopub.execute_input":"2024-10-15T14:58:05.523417Z","iopub.status.idle":"2024-10-15T14:58:36.890479Z","shell.execute_reply.started":"2024-10-15T14:58:05.523379Z","shell.execute_reply":"2024-10-15T14:58:36.889274Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Inference Time: 31361.442 ms\ntable, in less than 100 words, 3 most impressive final matches of the World Cup?\n\nHere are three of the most impressive final matches of the World Cup:\n\n1. **1982: West Germany vs. Italy:** A tense, tactical battle that saw West Germany emerge victorious with a 3-1 win.  \n2. **2006: Italy vs. France:** A thrilling match that went down to the wire, with Italy ultimately winning 5-3 on penalties after a 1-1 draw.\n3. **2018: France vs. Croatia\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Demo-2 Imitation (Japanese)**","metadata":{}},{"cell_type":"code","source":"start_time = time.perf_counter()\n\nresults = gemma_lm.generate(\"リスト, 250語以内で, 桜の季節におすすめの青森のお花見スポット?\", max_length=256)\n\nend_time = time.perf_counter() \nexecution_time = (end_time - start_time) * 1000 \nprint(f\"Inference Time: {execution_time:.3f} ms\")\n\nprint(results)","metadata":{"id":"VEyTnnNGvgGG","outputId":"bb60544b-52a1-460e-9cf4-44225a9334f2","execution":{"iopub.status.busy":"2024-10-15T15:16:24.143814Z","iopub.execute_input":"2024-10-15T15:16:24.144878Z","iopub.status.idle":"2024-10-15T15:17:36.005794Z","shell.execute_reply.started":"2024-10-15T15:16:24.144842Z","shell.execute_reply":"2024-10-15T15:17:36.004665Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Inference Time: 71855.625 ms\nリスト, 250語以内で, 桜の季節におすすめの青森のお花見スポット?\n\n## 桜の季節におすすめの青森のお花見スポット\n\n青森県は、桜の名所としても知られています。美しい桜並木が点在するスポットで、春の訪れを満喫できます。ここでは、おすすめの桜の季節におすすめのスポットを250語以内でご紹介します。\n\n**1. 青森県立美術館:**  美術館の庭園には、約100本の桜が咲き誇ります。美術館の雰囲気を楽しみながら、桜を眺められます。\n\n**2. 青森県庁舎:**  県庁舎の庭園には、約100本の桜が咲き誇ります。県庁舎の美しい建築物と桜の景色が、まさに芸術作品のような光景です。\n\n**3. 青森県立博物館:**  博物館の庭園には、約100本の桜が咲き誇ります。博物館の展示物と桜の景色を楽しみながら、歴史に触れることができます。\n\n**4. 青森市公園:**  市内にある公園には、約100本の桜が咲き誇ります。公園の緑豊かな自然と桜の景色が、心身ともにリフレッシュ\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### <font color=red>**Attention:** The answers are less relevant to the question</font> \n* <font color=blue>Consider a **relevance** **model** for **ranking** to pick the most relevant answers from proposals</font>","metadata":{}},{"cell_type":"markdown","source":"If you're running on JAX or TensorFlow backends, you'll notice that the second `generate` call returns nearly instantly. This is because each call to `generate` for a given batch size and `max_length` is compiled with XLA. The first run is expensive, but subsequent runs are much faster.","metadata":{"id":"vVlCnY7Gvm7U"}},{"cell_type":"markdown","source":"**Demo-3:** You can also provide batched prompts using a list as input:","metadata":{"id":"mw5XkiHU11Ft"}},{"cell_type":"code","source":"start_time = time.perf_counter()\n\nresults = gemma_lm.generate(\n    [\"what is keras in 3 bullet points?\",\n     \"list, in less than 100 words, 5 things you can help me do?\"],\n    max_length=128)\n\nend_time = time.perf_counter() \nexecution_time = (end_time - start_time) * 1000 \nprint(f\"Inference Time: {execution_time:.3f} ms\")\n\nprint(results)","metadata":{"id":"xV6vs8_C2BGt","outputId":"821a19a9-8ede-4662-aac5-8917b827053b","execution":{"iopub.status.busy":"2024-10-15T15:20:55.274546Z","iopub.execute_input":"2024-10-15T15:20:55.275535Z","iopub.status.idle":"2024-10-15T15:22:40.442452Z","shell.execute_reply.started":"2024-10-15T15:20:55.275501Z","shell.execute_reply":"2024-10-15T15:22:40.441405Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"WARNING:tensorflow:5 out of the last 8 calls to <bound method GemmaCausalLM.generate_step of <GemmaCausalLM name=gemma_causal_lm, built=True>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\nInference Time: 105162.119 ms\n['what is keras in 3 bullet points?\\n\\n* **High-level API:** Keras provides a user-friendly interface for building and training neural networks, making it accessible to both beginners and experts.\\n* **Flexible and Extensible:** Keras is built on top of TensorFlow, offering a wide range of customization options and the ability to integrate with other libraries.\\n* **Production-ready:** Keras models can be deployed and used in real-world applications, including image classification, object detection, and natural language processing. \\n<end_of_turn>', 'list, in less than 100 words, 5 things you can help me do?\\n\\n1. **Write a poem about a lost cat.**\\n2. **Create a list of fun activities for a rainy day.**\\n3. **Generate a funny joke.**\\n4. **Help me brainstorm ideas for a birthday party.**\\n5. **Explain a complex scientific concept in simple terms.** \\n\\n\\n<end_of_turn>']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Demo-3 Imitation (Spanish)**","metadata":{"id":"mw5XkiHU11Ft"}},{"cell_type":"code","source":"start_time = time.perf_counter()\n\nresults = gemma_lm.generate(\n    [\"El mejor gol de Di María en su carrera?\",\n     \"lista, en menos de 250 palabras, Los 5 mejores goles de Di María en su carrera?\"],\n    max_length=256)\n\nend_time = time.perf_counter() \nexecution_time = (end_time - start_time) * 1000 \nprint(f\"Inference Time: {execution_time:.3f} ms\")\n\nprint(results)","metadata":{"id":"xV6vs8_C2BGt","outputId":"821a19a9-8ede-4662-aac5-8917b827053b","execution":{"iopub.status.busy":"2024-10-15T15:31:22.347781Z","iopub.execute_input":"2024-10-15T15:31:22.349216Z","iopub.status.idle":"2024-10-15T15:35:22.197000Z","shell.execute_reply.started":"2024-10-15T15:31:22.349144Z","shell.execute_reply":"2024-10-15T15:35:22.196068Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Inference Time: 239840.454 ms\n['El mejor gol de Di María en su carrera?\\n\\nEl mejor gol de Di María en su carrera es un tema de opinión, ya que hay muchos que podrían considerarse como candidatos. \\n\\nSin embargo, algunos de los goles más populares y recordadas incluyen:\\n\\n* **Su golazo contra el Real Madrid en la Copa de la Liga de 2019.** Este gol, con un tiro cruzado desde la frontal del área, fue considerado un verdadero espectáculo de técnica y precisión.\\n* **Su gol de tiro libre contra el Atlético Madrid en la temporada 2017-18.** Este gol, con una ejecución impecable, marcó el 3-0 y sentó la base de una victoria contundente.\\n* **Su gol de chilena contra el Sevilla en la temporada 2016-17.** Este gol, con una ejecución magistral, fue un verdadero golazo y demostró la calidad y versatilidad técnica de Di María.\\n* **Su gol de chilena contra el Bayern Munich en la temporada 2014-15.** Este gol, con una ejecución impresionante, fue uno de los goles más memorables del año.\\n\\nEn definitiva, el mejor gol de Di María es subjetivamente el más', 'lista, en menos de 250 palabras, Los 5 mejores goles de Di María en su carrera? \\n\\n## Los 5 mejores goles de Di María\\n\\nConozca algunos de los mejores momentos en los que Lionel Messi Di María marcó goles.\\n\\n1. **El \"Hat Trick\" en la final de Copa Sudamericana 2014:** Di María fue fundamental en la victoria del Real Madrid sobre el Argentinos Juniors. Con un triplete, incluyendo un golazo con el pie, Di María demostró su talento y liderazgo en ese partido.\\n\\n2. **El \"Gol de la Historia\" contra el Barcelona en 2014:** Di María marcó un golazo con la pierna derecha en el Bernabéu. Este gol, considerado por muchos como el mejor de su carrera, sentó la base para la remontada del Real Madrid.\\n\\n3. **El \"Gol del Desgarrador\" contra el Liverpool en 2014:** Di María marcó un gol con una potente tiro que se convirtió en la pieza clave para la victoria del Real Madrid. Este gol se caracterizó por su fuerza y precisión.\\n\\n4. **El \"Gol de la Alegría\" en el PSG:** Di María anotó un']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Optional: Try a different sampler\n\nYou can control the generation strategy for `GemmaCausalLM` by setting the `sampler` argument on `compile()`. By default, [`\"greedy\"`](https://keras.io/api/keras_nlp/samplers/greedy_sampler/#greedysampler-class) sampling will be used.\n\n**Demo-4:** As an experiment, try setting a [`\"top_k\"`](https://keras.io/api/keras_nlp/samplers/top_k_sampler/) strategy:","metadata":{"id":"MaVWoSpo3XyY"}},{"cell_type":"code","source":"start_time = time.perf_counter()\n\ngemma_lm.compile(sampler=\"top_k\")\nresults = gemma_lm.generate(\"list, in less than 100 words, 5 things you can help me do?\", max_length=128)\n\nend_time = time.perf_counter() \nexecution_time = (end_time - start_time) * 1000 \nprint(f\"Inference Time: {execution_time:.3f} ms\")\n\nprint(results)","metadata":{"id":"mx55VQpN4DAK","outputId":"01698ab5-4e3b-4eb8-9bed-d99de63c05c9","execution":{"iopub.status.busy":"2024-10-15T15:26:08.716646Z","iopub.execute_input":"2024-10-15T15:26:08.717833Z","iopub.status.idle":"2024-10-15T15:26:42.329388Z","shell.execute_reply.started":"2024-10-15T15:26:08.717796Z","shell.execute_reply":"2024-10-15T15:26:42.328388Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"WARNING:tensorflow:6 out of the last 9 calls to <bound method GemmaCausalLM.generate_step of <GemmaCausalLM name=gemma_causal_lm, built=True>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\nInference Time: 33606.445 ms\nlist, in less than 100 words, 5 things you can help me do?\n\n1. **Summarize a long text.**\n2. **Translate languages.**\n3. **Write different kinds of creative text formats.**\n4. **Answer questions about a wide variety of topics.**\n5. **Provide information about different cultures.** \n\n\nLet me know if you are up to the task!\n<end_of_turn>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Demo-4 Imitation (German)**","metadata":{}},{"cell_type":"code","source":"start_time = time.perf_counter()\n\ngemma_lm.compile(sampler=\"top_k\")\nresults = gemma_lm.generate(\"liste, in SST enthält 250 Wörter, Die 5 besten Spiele der Euro 2024?\", max_length=256)\n\nend_time = time.perf_counter() \nexecution_time = (end_time - start_time) * 1000 \nprint(f\"Inference Time: {execution_time:.3f} ms\")\n\nprint(results)","metadata":{"id":"mx55VQpN4DAK","outputId":"01698ab5-4e3b-4eb8-9bed-d99de63c05c9","execution":{"iopub.status.busy":"2024-10-15T15:38:57.296891Z","iopub.execute_input":"2024-10-15T15:38:57.297304Z","iopub.status.idle":"2024-10-15T15:40:12.929690Z","shell.execute_reply.started":"2024-10-15T15:38:57.297271Z","shell.execute_reply":"2024-10-15T15:40:12.928686Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Inference Time: 75627.002 ms\nliste, in SST enthält 250 Wörter, Die 5 besten Spiele der Euro 2024?\n\n**Top 5 Euro 2024-Spiele:**\n\nDie Euro 2024 wird mit Spannung erwartet und es ist schwer, die Top 5-Spiele zu bestimmen, aber hier sind einige, die definitiv ein Highlight werden könnten:\n\n1.  **Deutschland - England**: Die Klassiker der europäischen Fußballgeschichte treffen sich in der Gruppe! Diese Begegnung wird mit Spannung erwartet und ist eine klare Favoriten-Wahl mit einer starken deutschen Mannschaft.\n2.  **Frankreich - Belgien**: Die beiden französischen Nachbarn liefern sich eine spannende Partie mit viel Emotionen! Frankreich hat die Qualifikation mit einer dominant-en Leistung erreicht und Belgien ist immer eine ernstzunehmende Gegnerin.\n3.  **Niederlande - Italien**: Die Niederlande sind mit einem jungen und dynamischen Team eine der größten Überraschungen der Vergangenheit. Italy ist ein echter Herausforderer mit einer erfahrenen und starken Nationalmannschaft.\n4.  **Spanien - Portugal**: Die beiden europäischen Gigantenteams treffen sich in einer hochkarätigen Begegnung! Portugal ist bekannt für seine starke Offensive, während Spanien eine starke Verteidigung\n","output_type":"stream"}]},{"cell_type":"markdown","source":"While the default greedy algorithm always picks the token with the largest probability, the top-K algorithm randomly picks the next token from the tokens of top K probability.\n\nYou don't have to specify a sampler, and you can ignore the last code snippet if it's not helpful to your use case. If you'd like learn more about the available samplers, see [Samplers](https://keras.io/api/keras_nlp/samplers/).","metadata":{"id":"-okKgK4LfO0f"}},{"cell_type":"markdown","source":"## What's next\n\nIn this tutorial, you learned how to generate text using KerasNLP and Gemma. Here are a few suggestions for what to learn next:\n\n* Learn how to [finetune a Gemma model](https://ai.google.dev/gemma/docs/lora_tuning).\n* Learn how to perform [distributed fine-tuning and inference on a Gemma model](https://ai.google.dev/gemma/docs/distributed_tuning).\n* Learn about [Gemma integration with Vertex AI](https://ai.google.dev/gemma/docs/integrations/vertex)\n* Learn how to [use Gemma models with Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/open-models/use-gemma).","metadata":{"id":"jBrbTYasoo-J"}}]}