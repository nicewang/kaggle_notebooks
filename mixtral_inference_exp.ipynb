{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook is for Performance Evaluation for different GPU Accelerator","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\n\nhuggingface_key = \"xxx...xxx\"\nlogin(huggingface_key)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-09T13:25:21.103980Z","iopub.execute_input":"2024-10-09T13:25:21.104828Z","iopub.status.idle":"2024-10-09T13:25:21.652467Z","shell.execute_reply.started":"2024-10-09T13:25:21.104770Z","shell.execute_reply":"2024-10-09T13:25:21.651578Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install flash-attn --no-build-isolation","metadata":{"execution":{"iopub.status.busy":"2024-10-09T13:25:26.424975Z","iopub.execute_input":"2024-10-09T13:25:26.425352Z","iopub.status.idle":"2024-10-09T13:25:58.191059Z","shell.execute_reply.started":"2024-10-09T13:25:26.425315Z","shell.execute_reply":"2024-10-09T13:25:58.190132Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting flash-attn\n  Downloading flash_attn-2.6.3.tar.gz (2.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash-attn) (2.4.0)\nCollecting einops (from flash-attn)\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: flash-attn\n  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.6.3-cp310-cp310-linux_x86_64.whl size=187315346 sha256=6ebfbdcbdd164f80278a954d29a1bc9d620130264215f0fb93374a6ab4e0a283\n  Stored in directory: /root/.cache/pip/wheels/7e/e3/c3/89c7a2f3c4adc07cd1c675f8bb7b9ad4d18f64a72bccdfe826\nSuccessfully built flash-attn\nInstalling collected packages: einops, flash-attn\nSuccessfully installed einops-0.8.0 flash-attn-2.6.3\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():\n    print(\"GPU is available\")\nelse:\n    print(\"GPU is not available\")","metadata":{"execution":{"iopub.status.busy":"2024-10-09T13:26:16.846725Z","iopub.execute_input":"2024-10-09T13:26:16.847144Z","iopub.status.idle":"2024-10-09T13:26:18.641566Z","shell.execute_reply.started":"2024-10-09T13:26:16.847104Z","shell.execute_reply":"2024-10-09T13:26:18.640451Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"GPU is available\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom tqdm import tqdm\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"mistralai/Mixtral-8x7B-v0.1\", \n    torch_dtype=torch.float16, \n    device_map=\"auto\",\n    offload_folder=\"offload\"\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n\nprompt = \"My favourite condiment is\"\nmodel_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T13:26:22.439485Z","iopub.execute_input":"2024-10-09T13:26:22.440008Z","iopub.status.idle":"2024-10-09T13:36:44.890212Z","shell.execute_reply.started":"2024-10-09T13:26:22.439969Z","shell.execute_reply":"2024-10-09T13:36:44.889391Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68fd4274df07416fa811cc9e828d83a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/92.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcc27f0370b04e53892446c22b4fe5c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/19 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d94507b1c7642aab7ac1ffd16488e38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00019.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"564ac8647899480381c859c0356cae2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a069fd77acbd405d8635de6703e5d345"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a956880dfd0449699e6979b040471c18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dab98a84d1ed4dec9d79ba678ca6212f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3abc6835d454676af569dd43ede32ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00006-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b6916f65095480a87693d3a4dfcddbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00007-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"448f1097642c416ca68d84ee03de829e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00008-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea0bf4f7805f4a0ca25eddf5939903f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00009-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aeb2809a637c42d0860babe97e8a0230"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00010-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5128874e40234e9f9b8ca970c1d141e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00011-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cea32fc2e6d547febd6e5c9e95af8526"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00012-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b09fb3960414ad5b02d9ca189982015"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00013-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7269642852984e0e817005e1d903a7df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00014-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bee9a966c18a46e8a14fb0d4b54c5b34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00015-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71928f14047f46de9de278b06c59a33b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00016-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52b1886006b243f69f4112cae00c566d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00017-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"328c2cff97d54c53a53385c0e976f266"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00018-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80ed8114892f499f9f34e403ce4a412f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00019-of-00019.safetensors:   0%|          | 0.00/4.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba2a52324d464e4db664f19a544c6cb4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/utils/modeling.py:1462: UserWarning: Current model requires 469765632 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee562456109e49b681d612a615aeae5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32c290689cb447ba855267c05538e5b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"351593466b4644e6bef62e955f4089ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7aef4c328a64f238f57096d88846618"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08eee869261548acbb2b5bf787f0fd7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32ec1ab91ad4419bb30f139ee523e515"}},"metadata":{}}]},{"cell_type":"markdown","source":"## 1. Accelerator: GPU T4 x 2\n* each GPU's memory: 15G","metadata":{}},{"cell_type":"markdown","source":"### Generate 5 new tokens","metadata":{}},{"cell_type":"code","source":"output_sequences = []\nfor i in tqdm(range(4)):\n    with torch.amp.autocast('cuda'):\n        generated_ids = model.generate(**model_inputs, max_new_tokens=5, do_sample=True)\n        output_sequences.append(generated_ids)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T04:12:51.675202Z","iopub.execute_input":"2024-10-07T04:12:51.675614Z","iopub.status.idle":"2024-10-07T05:30:10.437792Z","shell.execute_reply.started":"2024-10-07T04:12:51.675577Z","shell.execute_reply":"2024-10-07T05:30:10.436928Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"  0%|          | 0/4 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 25%|██▌       | 1/4 [18:28<55:24, 1108.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 50%|█████     | 2/4 [38:10<38:24, 1152.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 75%|███████▌  | 3/4 [57:51<19:25, 1165.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n100%|██████████| 4/4 [1:17:18<00:00, 1159.69s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"output_sequences = torch.cat(output_sequences, dim=1)\n\ndecoded_output = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)[0]\nprint(decoded_output)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T05:34:09.745507Z","iopub.execute_input":"2024-10-07T05:34:09.746124Z","iopub.status.idle":"2024-10-07T05:34:09.750637Z","shell.execute_reply.started":"2024-10-07T05:34:09.746087Z","shell.execute_reply":"2024-10-07T05:34:09.749668Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"My favourite condiment is the one I cannot pron My favourite condiment is a toss up between must My favourite condiment is Sriracha hot My favourite condiment is hot sauce.  I\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Generate 10 new tokens","metadata":{}},{"cell_type":"code","source":"output_sequences = []\nfor i in tqdm(range(2)):\n    with torch.amp.autocast('cuda'):\n        generated_ids = model.generate(**model_inputs, max_new_tokens=10, do_sample=True)\n        output_sequences.append(generated_ids)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T14:36:49.332516Z","iopub.execute_input":"2024-10-07T14:36:49.333071Z","iopub.status.idle":"2024-10-07T15:54:05.450380Z","shell.execute_reply.started":"2024-10-07T14:36:49.333031Z","shell.execute_reply":"2024-10-07T15:54:05.449209Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"  0%|          | 0/2 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n 50%|█████     | 1/2 [38:24<38:24, 2304.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n100%|██████████| 2/2 [1:17:16<00:00, 2318.05s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"output_sequences = torch.cat(output_sequences, dim=1)\n\ndecoded_output = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)[0]\nprint(decoded_output)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T15:54:52.327509Z","iopub.execute_input":"2024-10-07T15:54:52.328408Z","iopub.status.idle":"2024-10-07T15:54:52.341266Z","shell.execute_reply.started":"2024-10-07T15:54:52.328365Z","shell.execute_reply":"2024-10-07T15:54:52.340464Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"My favourite condiment is mustard. My favourite animal is a cow. My favourite condiment is mayonnaise and with summer fast approaching I am\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Generate 20 new tokens","metadata":{}},{"cell_type":"code","source":"output_sequences = []\nfor i in tqdm(range(1)):\n    with torch.amp.autocast('cuda'):\n        generated_ids = model.generate(**model_inputs, max_new_tokens=20, do_sample=True)\n        output_sequences.append(generated_ids)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T09:21:21.678830Z","iopub.execute_input":"2024-10-08T09:21:21.679875Z","iopub.status.idle":"2024-10-08T10:31:36.665515Z","shell.execute_reply.started":"2024-10-08T09:21:21.679829Z","shell.execute_reply":"2024-10-08T10:31:36.663643Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"  0%|          | 0/1 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n100%|██████████| 1/1 [1:10:14<00:00, 4214.97s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"output_sequences = torch.cat(output_sequences, dim=1)\n\ndecoded_output = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)[0]\nprint(decoded_output)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T10:33:38.484444Z","iopub.execute_input":"2024-10-08T10:33:38.485656Z","iopub.status.idle":"2024-10-08T10:33:38.494045Z","shell.execute_reply.started":"2024-10-08T10:33:38.485611Z","shell.execute_reply":"2024-10-08T10:33:38.493167Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"My favourite condiment is without a doubt a good ol’ fashioned salsa – you might have noticed the many recipes\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2. Accelerator: GPU P100 x 1\n* GPU's memory: 16G","metadata":{}},{"cell_type":"markdown","source":"### Generate 5 new tokens","metadata":{}},{"cell_type":"code","source":"output_sequences = []\nfor i in tqdm(range(4)):\n    with torch.amp.autocast('cuda'):\n        generated_ids = model.generate(**model_inputs, max_new_tokens=5, do_sample=True)\n        output_sequences.append(generated_ids)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T13:37:32.875991Z","iopub.execute_input":"2024-10-09T13:37:32.877045Z","iopub.status.idle":"2024-10-09T15:17:39.143416Z","shell.execute_reply.started":"2024-10-09T13:37:32.876989Z","shell.execute_reply":"2024-10-09T15:17:39.142387Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"  0%|          | 0/4 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n 25%|██▌       | 1/4 [24:02<1:12:06, 1442.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 50%|█████     | 2/4 [49:24<49:38, 1489.08s/it]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 75%|███████▌  | 3/4 [1:14:34<24:58, 1498.92s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n100%|██████████| 4/4 [1:40:06<00:00, 1501.56s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"output_sequences = torch.cat(output_sequences, dim=1)\n\ndecoded_output = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)[0]\nprint(decoded_output)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T15:20:44.336998Z","iopub.execute_input":"2024-10-09T15:20:44.338228Z","iopub.status.idle":"2024-10-09T15:20:44.348057Z","shell.execute_reply.started":"2024-10-09T15:20:44.338186Z","shell.execute_reply":"2024-10-09T15:20:44.347146Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"My favourite condiment is a tie between mayonna My favourite condiment is chilli flakes in My favourite condiment is ketchup and for My favourite condiment is hummus. It doesn\n","output_type":"stream"}]}]}