{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook is for Performance Evaluation for different GPU Accelerator","metadata":{}},{"cell_type":"markdown","source":"## 1. Accelerator: GPU T4 x 2\n* each GPU's memory: 16G","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\n\n# huggingface_key = \"xxx...xxx\"\n# login(huggingface_key)\nlogin(\"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\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-07T14:23:23.120128Z","iopub.execute_input":"2024-10-07T14:23:23.120506Z","iopub.status.idle":"2024-10-07T14:23:23.691422Z","shell.execute_reply.started":"2024-10-07T14:23:23.120463Z","shell.execute_reply":"2024-10-07T14:23:23.690395Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install flash-attn --no-build-isolation","metadata":{"execution":{"iopub.status.busy":"2024-10-07T14:23:28.635992Z","iopub.execute_input":"2024-10-07T14:23:28.636814Z","iopub.status.idle":"2024-10-07T14:23:59.176786Z","shell.execute_reply.started":"2024-10-07T14:23:28.636774Z","shell.execute_reply":"2024-10-07T14:23:59.175691Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting flash-attn\n  Downloading flash_attn-2.6.3.tar.gz (2.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash-attn) (2.4.0)\nCollecting einops (from flash-attn)\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: flash-attn\n  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.6.3-cp310-cp310-linux_x86_64.whl size=187315346 sha256=6ebfbdcbdd164f80278a954d29a1bc9d620130264215f0fb93374a6ab4e0a283\n  Stored in directory: /root/.cache/pip/wheels/7e/e3/c3/89c7a2f3c4adc07cd1c675f8bb7b9ad4d18f64a72bccdfe826\nSuccessfully built flash-attn\nInstalling collected packages: einops, flash-attn\nSuccessfully installed einops-0.8.0 flash-attn-2.6.3\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():\n    print(\"GPU is available\")\nelse:\n    print(\"GPU is not available\")","metadata":{"execution":{"iopub.status.busy":"2024-10-07T14:24:17.994027Z","iopub.execute_input":"2024-10-07T14:24:17.994772Z","iopub.status.idle":"2024-10-07T14:24:19.696636Z","shell.execute_reply.started":"2024-10-07T14:24:17.994726Z","shell.execute_reply":"2024-10-07T14:24:19.695645Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"GPU is available\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom tqdm import tqdm\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"mistralai/Mixtral-8x7B-v0.1\", \n    torch_dtype=torch.float16, \n    device_map=\"auto\",\n    offload_folder=\"offload\"\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n\nprompt = \"My favourite condiment is\"\nmodel_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T14:24:24.312254Z","iopub.execute_input":"2024-10-07T14:24:24.312745Z","iopub.status.idle":"2024-10-07T14:35:38.868105Z","shell.execute_reply.started":"2024-10-07T14:24:24.312705Z","shell.execute_reply":"2024-10-07T14:35:38.867065Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84f51a20111a4dc8bb1585949ec39e96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/92.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40d6dfa9af304428ab928e10b18e41fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/19 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed28545a907b4cb7be21e3f4112db2f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00019.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1653b838ee2a4cfc8255a667179d71d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbe143be43a94b6d84b47344b9103ca2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd733462236c45d49d818f769c95263e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"daf68828e9cb45638d2866d26fbfe04e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05917483837d4fa9ae73c640d753897c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00006-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7650a8da7ca44381af77eb30f45d94dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00007-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b3067912972485d97f1a8418ab0e402"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00008-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96d50ac89a044079bdddf248f4d0c432"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00009-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea32e66bbd674f97bb7e3527bc088162"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00010-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dea45f79b4247079bf014971bb64323"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00011-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5edbd8dfd5e471391a48d7d126629f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00012-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb2f1ad6c9d54f10a7ff1523a95574b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00013-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30f905084ae844e9883492de62c21a5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00014-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea2c4dd1becc48d29173663564e373b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00015-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"755734c2484e44c88623200be2530b15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00016-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ff1449745a042759f9cc0b92c89e7ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00017-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8e1bfcdb02247e7ba8b45206126a44a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00018-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0886697bf18e4740850066223d24e804"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00019-of-00019.safetensors:   0%|          | 0.00/4.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29c525cd8a5b43718cbbebd3d91f9825"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b22019ecd67d4ceb9c4f823e59e2a8f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b83e8428976d48d28558cb630d5bf9f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a807e3746af043c6bbe25359580b4102"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d4835f7f7a949af80cb3299d7690a41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d015d8d5dafd4bd4a4f5158e419520aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e9c74357822425c9e4b9f491f18d112"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Generate 5 new tokens","metadata":{}},{"cell_type":"code","source":"output_sequences = []\nfor i in tqdm(range(4)):\n    with torch.amp.autocast('cuda'):\n        generated_ids = model.generate(**model_inputs, max_new_tokens=5, do_sample=True)\n        output_sequences.append(generated_ids)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T04:12:51.675202Z","iopub.execute_input":"2024-10-07T04:12:51.675614Z","iopub.status.idle":"2024-10-07T05:30:10.437792Z","shell.execute_reply.started":"2024-10-07T04:12:51.675577Z","shell.execute_reply":"2024-10-07T05:30:10.436928Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"  0%|          | 0/4 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 25%|██▌       | 1/4 [18:28<55:24, 1108.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 50%|█████     | 2/4 [38:10<38:24, 1152.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 75%|███████▌  | 3/4 [57:51<19:25, 1165.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n100%|██████████| 4/4 [1:17:18<00:00, 1159.69s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"output_sequences = torch.cat(output_sequences, dim=1)\n\ndecoded_output = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)[0]\nprint(decoded_output)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T05:34:09.745507Z","iopub.execute_input":"2024-10-07T05:34:09.746124Z","iopub.status.idle":"2024-10-07T05:34:09.750637Z","shell.execute_reply.started":"2024-10-07T05:34:09.746087Z","shell.execute_reply":"2024-10-07T05:34:09.749668Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"My favourite condiment is the one I cannot pron My favourite condiment is a toss up between must My favourite condiment is Sriracha hot My favourite condiment is hot sauce.  I\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Generate 10 new tokens","metadata":{}},{"cell_type":"code","source":"output_sequences = []\nfor i in tqdm(range(2)):\n    with torch.amp.autocast('cuda'):\n        generated_ids = model.generate(**model_inputs, max_new_tokens=10, do_sample=True)\n        output_sequences.append(generated_ids)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T14:36:49.332516Z","iopub.execute_input":"2024-10-07T14:36:49.333071Z","iopub.status.idle":"2024-10-07T15:54:05.450380Z","shell.execute_reply.started":"2024-10-07T14:36:49.333031Z","shell.execute_reply":"2024-10-07T15:54:05.449209Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"  0%|          | 0/2 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n 50%|█████     | 1/2 [38:24<38:24, 2304.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n100%|██████████| 2/2 [1:17:16<00:00, 2318.05s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"output_sequences = torch.cat(output_sequences, dim=1)\n\ndecoded_output = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)[0]\nprint(decoded_output)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T15:54:52.327509Z","iopub.execute_input":"2024-10-07T15:54:52.328408Z","iopub.status.idle":"2024-10-07T15:54:52.341266Z","shell.execute_reply.started":"2024-10-07T15:54:52.328365Z","shell.execute_reply":"2024-10-07T15:54:52.340464Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"My favourite condiment is mustard. My favourite animal is a cow. My favourite condiment is mayonnaise and with summer fast approaching I am\n","output_type":"stream"}]}]}