{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook is for Performance Evaluation for different GPU Accelerator","metadata":{}},{"cell_type":"markdown","source":"## 1. Accelerator: GPU T4 x 2\n* each GPU's memory: 16G","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\n\nhuggingface_key = \"xxx...xxx\"\nlogin(huggingface_key)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-08T09:03:53.820985Z","iopub.execute_input":"2024-10-08T09:03:53.821797Z","iopub.status.idle":"2024-10-08T09:03:54.404724Z","shell.execute_reply.started":"2024-10-08T09:03:53.821754Z","shell.execute_reply":"2024-10-08T09:03:54.403835Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install flash-attn --no-build-isolation","metadata":{"execution":{"iopub.status.busy":"2024-10-08T09:04:45.717776Z","iopub.execute_input":"2024-10-08T09:04:45.718640Z","iopub.status.idle":"2024-10-08T09:05:17.392740Z","shell.execute_reply.started":"2024-10-08T09:04:45.718598Z","shell.execute_reply":"2024-10-08T09:05:17.391621Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting flash-attn\n  Downloading flash_attn-2.6.3.tar.gz (2.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash-attn) (2.4.0)\nCollecting einops (from flash-attn)\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: flash-attn\n  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.6.3-cp310-cp310-linux_x86_64.whl size=187315346 sha256=6ebfbdcbdd164f80278a954d29a1bc9d620130264215f0fb93374a6ab4e0a283\n  Stored in directory: /root/.cache/pip/wheels/7e/e3/c3/89c7a2f3c4adc07cd1c675f8bb7b9ad4d18f64a72bccdfe826\nSuccessfully built flash-attn\nInstalling collected packages: einops, flash-attn\nSuccessfully installed einops-0.8.0 flash-attn-2.6.3\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():\n    print(\"GPU is available\")\nelse:\n    print(\"GPU is not available\")","metadata":{"execution":{"iopub.status.busy":"2024-10-08T09:05:55.433064Z","iopub.execute_input":"2024-10-08T09:05:55.433986Z","iopub.status.idle":"2024-10-08T09:05:57.154458Z","shell.execute_reply.started":"2024-10-08T09:05:55.433943Z","shell.execute_reply":"2024-10-08T09:05:57.153467Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"GPU is available\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom tqdm import tqdm\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"mistralai/Mixtral-8x7B-v0.1\", \n    torch_dtype=torch.float16, \n    device_map=\"auto\",\n    offload_folder=\"offload\"\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n\nprompt = \"My favourite condiment is\"\nmodel_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T09:06:05.298663Z","iopub.execute_input":"2024-10-08T09:06:05.299171Z","iopub.status.idle":"2024-10-08T09:17:14.571124Z","shell.execute_reply.started":"2024-10-08T09:06:05.299133Z","shell.execute_reply":"2024-10-08T09:17:14.570327Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39f707e22dd744b2a151c3e47ce1a1b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/92.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61d63d7688d24c01801385c1f7b9d3a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/19 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2aed3de17b05444ab76aca22e26166bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00019.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1590d54f055473cb0367dead9e0af6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87b2eb6573f745f68f8a69fc5dfb89fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66d4e26f7d394575a653655ac2b8d8de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff2620de4af74edb9f1cdf1ca77b8656"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0217847e671421aaa305f459d001b10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00006-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d488d54f8a4f4a7caee3bbe04d013caa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00007-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6002f0e7f13248e18207c12aaf998c8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00008-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"558219d741d04d01be1a8728ca4a1bb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00009-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddebafb6609f43fc836bdf9ef1175842"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00010-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71e1b3430ad24c35a54317b2c45f64ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00011-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a47146950f6488d8f8c865b86e5b073"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00012-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5d3a40f6d754edbb8891d766132360b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00013-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b52a3e8a908f403e921ebdabb55a83bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00014-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84a2555c74044c0682af430acdb10946"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00015-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c87135432a31436c8d5eecf38eeac95f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00016-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4f13f660444410c8eb4dc495d1af0eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00017-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"614127a487db4e338ab95e7486be11ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00018-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8cf7c9e41cc4307a22bcbfe7909aee1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00019-of-00019.safetensors:   0%|          | 0.00/4.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"211920308e814a2f973bb558f72ffb6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24bd2c24389f4063a3efaea76623642e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c25af27ffba746ae929371aa215ff4ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65abedcfce1d4ff782b4fdca96328fdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c420e1e10db4860b6e87167f2d7215c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d37424e004f64276ad7d5c2bc39b410d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb4b915bcf4d45ac8da4e61fd448be1f"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Generate 5 new tokens","metadata":{}},{"cell_type":"code","source":"output_sequences = []\nfor i in tqdm(range(4)):\n    with torch.amp.autocast('cuda'):\n        generated_ids = model.generate(**model_inputs, max_new_tokens=5, do_sample=True)\n        output_sequences.append(generated_ids)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T04:12:51.675202Z","iopub.execute_input":"2024-10-07T04:12:51.675614Z","iopub.status.idle":"2024-10-07T05:30:10.437792Z","shell.execute_reply.started":"2024-10-07T04:12:51.675577Z","shell.execute_reply":"2024-10-07T05:30:10.436928Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"  0%|          | 0/4 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 25%|██▌       | 1/4 [18:28<55:24, 1108.11s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 50%|█████     | 2/4 [38:10<38:24, 1152.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n 75%|███████▌  | 3/4 [57:51<19:25, 1165.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n100%|██████████| 4/4 [1:17:18<00:00, 1159.69s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"output_sequences = torch.cat(output_sequences, dim=1)\n\ndecoded_output = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)[0]\nprint(decoded_output)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T05:34:09.745507Z","iopub.execute_input":"2024-10-07T05:34:09.746124Z","iopub.status.idle":"2024-10-07T05:34:09.750637Z","shell.execute_reply.started":"2024-10-07T05:34:09.746087Z","shell.execute_reply":"2024-10-07T05:34:09.749668Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"My favourite condiment is the one I cannot pron My favourite condiment is a toss up between must My favourite condiment is Sriracha hot My favourite condiment is hot sauce.  I\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Generate 10 new tokens","metadata":{}},{"cell_type":"code","source":"output_sequences = []\nfor i in tqdm(range(2)):\n    with torch.amp.autocast('cuda'):\n        generated_ids = model.generate(**model_inputs, max_new_tokens=10, do_sample=True)\n        output_sequences.append(generated_ids)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T14:36:49.332516Z","iopub.execute_input":"2024-10-07T14:36:49.333071Z","iopub.status.idle":"2024-10-07T15:54:05.450380Z","shell.execute_reply.started":"2024-10-07T14:36:49.333031Z","shell.execute_reply":"2024-10-07T15:54:05.449209Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"  0%|          | 0/2 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n 50%|█████     | 1/2 [38:24<38:24, 2304.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n100%|██████████| 2/2 [1:17:16<00:00, 2318.05s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"output_sequences = torch.cat(output_sequences, dim=1)\n\ndecoded_output = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)[0]\nprint(decoded_output)","metadata":{"execution":{"iopub.status.busy":"2024-10-07T15:54:52.327509Z","iopub.execute_input":"2024-10-07T15:54:52.328408Z","iopub.status.idle":"2024-10-07T15:54:52.341266Z","shell.execute_reply.started":"2024-10-07T15:54:52.328365Z","shell.execute_reply":"2024-10-07T15:54:52.340464Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"My favourite condiment is mustard. My favourite animal is a cow. My favourite condiment is mayonnaise and with summer fast approaching I am\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Generate 20 new tokens","metadata":{}},{"cell_type":"code","source":"output_sequences = []\nfor i in tqdm(range(1)):\n    with torch.amp.autocast('cuda'):\n        generated_ids = model.generate(**model_inputs, max_new_tokens=20, do_sample=True)\n        output_sequences.append(generated_ids)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T09:21:21.678830Z","iopub.execute_input":"2024-10-08T09:21:21.679875Z","iopub.status.idle":"2024-10-08T10:31:36.665515Z","shell.execute_reply.started":"2024-10-08T09:21:21.679829Z","shell.execute_reply":"2024-10-08T10:31:36.663643Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"  0%|          | 0/1 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n100%|██████████| 1/1 [1:10:14<00:00, 4214.97s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"output_sequences = torch.cat(output_sequences, dim=1)\n\ndecoded_output = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)[0]\nprint(decoded_output)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T10:33:38.484444Z","iopub.execute_input":"2024-10-08T10:33:38.485656Z","iopub.status.idle":"2024-10-08T10:33:38.494045Z","shell.execute_reply.started":"2024-10-08T10:33:38.485611Z","shell.execute_reply":"2024-10-08T10:33:38.493167Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"My favourite condiment is without a doubt a good ol’ fashioned salsa – you might have noticed the many recipes\n","output_type":"stream"}]}]}